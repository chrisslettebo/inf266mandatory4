{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "import imageio\n",
    "import glob\n",
    "import pickle\n",
    "import IPython\n",
    "import os\n",
    "from typing import Tuple, List\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "\n",
    "env = gymnasium.make('MountainCar-v0', max_episode_steps = 1000, render_mode=\"rgb_array\")\n",
    "env.action_space.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "#obs : position, velocity\n",
    "obs, info = env.reset() # initial observation\n",
    "print(obs)  \n",
    "print(info)  \n",
    "# take a step\n",
    "action = 2  # Push right\n",
    "obs, reward, terminated, truncated, info = env.step(action)\n",
    "print(obs, reward, terminated, truncated, info)\n",
    "#rendering\n",
    "frame = env.render()\n",
    "print(type(frame))  # <class 'numpy.ndarray'>\n",
    "plt.imshow(frame)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state space ranges from -1.2 to 0.6 for position and -0.07 to 0.07 for velocity\n",
    "# action space is discrete with 3 possible actions\n",
    "# reward is -1 for each time step until the goal position of 0.5 is reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_state(state, agent):\n",
    "    position, velocity = state\n",
    "    \n",
    "    # Create bins dynamically based on agent's n_bins\n",
    "    position_bins = np.linspace(-1.2, 0.6, agent.n_bins_position)\n",
    "    velocity_bins = np.linspace(-0.07, 0.07, agent.n_bins_velocity)\n",
    "\n",
    "    position_index = np.digitize(position, position_bins) - 1\n",
    "    velocity_index = np.digitize(velocity, velocity_bins) - 1\n",
    "\n",
    "    return (position_index, velocity_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Q table\n",
    "def initialize_q(n_bins_position, n_bins_velocity, n_actions):\n",
    "    n_actions = env.action_space.n\n",
    "    n_states = (n_bins_position, n_bins_velocity)\n",
    "    Q = np.zeros(n_states + (n_actions,))\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all the agents\n",
    "class BaseAgent(ABC):\n",
    "    def __init__(self, action_dim:int, epsilon: float, gamma: float = 1.0,n_bins_position:int=20,n_bins_velocity:int=20):\n",
    "        self.action_dim = action_dim\n",
    "        self.epsilon = self.initial_epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.n_bins_position = n_bins_position\n",
    "        self.n_bins_velocity = n_bins_velocity\n",
    "        self.Q = np.zeros((self.n_bins_position, self.n_bins_velocity, self.action_dim))  \n",
    "        \n",
    "\n",
    "    @abstractmethod\n",
    "    def select_action(self, state) -> int:\n",
    "        \"\"\"Select an action given the state\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def learn(self, *args, **kwargs) -> None:\n",
    "        \"\"\"Update Q-values based on learning method\"\"\"\n",
    "        pass\n",
    "    \n",
    "\n",
    "    def select_action(self, state) -> int:\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            return np.random.choice(self.action_dim)\n",
    "        else:\n",
    "            return self.greedy_action(state)\n",
    "\n",
    "    def greedy_action(self, state) -> int:\n",
    "        action_values = self.Q[state[0], state[1], :]\n",
    "        return np.argmax(action_values)  # Choose the best action\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.Q = np.zeros((31, 100, self.action_dim))  # Reset Q-table\n",
    "        self.epsilon = self.initial_epsilon\n",
    "\n",
    "    def __getstate__(self):\n",
    "        return self.__dict__.copy()\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        self.__dict__.update(state)\n",
    "\n",
    "\n",
    "class TD0Agent(BaseAgent):  \n",
    "    def __init__(self, action_dim, epsilon, alpha, gamma=1, offpolicy=False, n_bins_position=20, n_bins_velocity=20):\n",
    "        super().__init__(action_dim, epsilon, gamma, n_bins_position, n_bins_velocity)\n",
    "        self.offpolicy = offpolicy\n",
    "        self.alpha = self.initial_alpha = alpha\n",
    "\n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.alpha = self.initial_alpha\n",
    "\n",
    "        \n",
    "    def learn(self, experience:Experience)->None:\n",
    "        state, action, reward, next_state, done = experience\n",
    "        if self.offpolicy:\n",
    "            next_action = self.greedy_action(next_state) # q learning\n",
    "        else:\n",
    "            next_action = self.select_action(next_state) # sarsa\n",
    "\n",
    "        next_max = self.Q[next_state[0],next_state[1], next_action] \n",
    "    \n",
    "        self.Q[state][action] = self.Q[state][action] + self.alpha*(reward+  self.gamma*next_max - self.Q[state][action] )\n",
    "\n",
    "\n",
    "class NstepSarsaAgent(BaseAgent):\n",
    "    def __init__(self, action_dim, epsilon, alpha, gamma=1, n=3, n_bins_position=20, n_bins_velocity=20):\n",
    "        super().__init__(action_dim, epsilon, gamma, n_bins_position, n_bins_velocity)\n",
    "        self.alpha = self.initial_alpha = alpha\n",
    "        self.n = n  # Number of steps\n",
    "        self.memory = deque(maxlen=n+1)  # Store (state, action, reward)\n",
    "\n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.alpha = self.initial_alpha\n",
    "        self.memory.clear()\n",
    "\n",
    "    def learn(self, experience:Experience):\n",
    "        \"\"\"\n",
    "        Stores the current experience and updates Q-values when enough steps are collected.\n",
    "        \"\"\"\n",
    "        _, _, _, _, done = experience\n",
    "        self.memory.append(experience)\n",
    "\n",
    "       \n",
    "\n",
    "        if len(self.memory) < (self.n+1) and not done:  # if not reach terminal state: wait until enough steps are collected'\n",
    "            return\n",
    "\n",
    "        experiences = self.memory.copy()\n",
    "        while len(experiences) > 0:\n",
    "            G = 0 # Return\n",
    "            for i, (_, _, r, _,_) in enumerate(experiences):\n",
    "                G += (self.gamma ** (i+1)) * r  # Compute n-step return\n",
    "           \n",
    "            last_experience = experiences[-1]  # Last step\n",
    "            next_state = last_experience.next_state\n",
    "            next_action = self.select_action(next_state)\n",
    "            Q_n = self.Q[next_state[0], next_state[1], next_action]\n",
    "            G += (self.gamma ** self.n) * Q_n\n",
    "           \n",
    "            state, action, _, _, _ = experiences[0]\n",
    "            td_error = G - self.Q[state[0], state[1], action] # Temporal difference error r_1*gamma^0 + r_2*gamma^1 + ... + r_n*gamma^(n-1) + Q(S_n, A_n) - Q(S_0, A_0)\n",
    "            self.Q[state[0], state[1], action] += self.alpha * td_error\n",
    "            experiences.popleft()\n",
    "        \n",
    "        if len(self.memory) >= self.n + 1:\n",
    "            self.memory.popleft()\n",
    "        if done:\n",
    "            self.memory.clear()          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training loop\n",
    "def train_agent(agent, env, total_episodes=10000, decay_fn=None):\n",
    "    \"\"\"\n",
    "    Trains an agent (either MC or TD0) in MountainCar-v0.\n",
    "    \"\"\"\n",
    "    time_outs = 0\n",
    "    env.action_space.seed(42)\n",
    "\n",
    "    with tqdm(total=total_episodes, desc=\"Training Progress\") as pbar:\n",
    "        for episode in range(total_episodes):\n",
    "            state, _ = env.reset()\n",
    "            state = discretize_state(state, agent)  # Ensure it's discrete\n",
    "\n",
    "            done = False\n",
    "            step = 0\n",
    "            running_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                step += 1\n",
    "                action = agent.select_action(state)\n",
    "\n",
    "                # Take action\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                next_state = discretize_state(next_state,agent)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                running_reward += reward\n",
    "                experience = Experience(state, action, reward, next_state, done)\n",
    "                agent.learn(experience)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            # Apply decay to epsilon & alpha\n",
    "            if decay_fn:\n",
    "                agent.epsilon = decay_fn(episode, agent.initial_epsilon, total_episodes, A=0.5, B=0.2, C=0.001)\n",
    "                if isinstance(agent, (TD0Agent, NstepSarsaAgent)):\n",
    "                    agent.alpha = decay_fn(episode, agent.initial_alpha, total_episodes, A=0.5, B=0.2, C=0.00001)\n",
    "\n",
    "            # Logging\n",
    "            pbar.set_postfix({\"steps\": step, \"epsilon\": agent.epsilon, \"Running Return\": running_reward})\n",
    "            pbar.update(1)\n",
    "\n",
    "\n",
    "\n",
    "def get_trajectory(agent:BaseAgent,env:gym.Env, filename:str):\n",
    "    print(\"Creating video...\")\n",
    "    video_folder = \"videos\"\n",
    "    if not os.path.exists(video_folder):\n",
    "        os.makedirs(video_folder)\n",
    "    trajectory = []\n",
    "    env = gym.wrappers.RecordVideo(env, video_folder=video_folder, name_prefix=filename)\n",
    "    observation, info = env.reset()\n",
    "    observation = discretize_state(observation,agent)\n",
    "\n",
    "\n",
    "    while True:\n",
    "        env.render()\n",
    "        state = discretize_state(observation,agent)\n",
    "        trajectory.append(state)\n",
    "        action = agent.greedy_action(state)\n",
    "\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        observation = discretize_state(observation,agent)\n",
    "        if terminated or truncated:\n",
    "            env.render()\n",
    "            break\n",
    "    \n",
    "    state = discretize_state(observation,agent)\n",
    "    trajectory.append(state)\n",
    "    env.close()\n",
    "    print(\"Video saved.\")\n",
    "    video_files = sorted(\n",
    "        [f for f in os.listdir(video_folder) if f.endswith(\".mp4\") and f.startswith(filename)],\n",
    "        key=lambda f: os.path.getmtime(os.path.join(video_folder, f)),  # Sort by creation time\n",
    "        reverse=True\n",
    "    )\n",
    "    video_path = os.path.join(video_folder, video_files[0])\n",
    "    IPython.display.display(IPython.display.Video(video_path))\n",
    "    return trajectory\n",
    "\n",
    "\n",
    "\n",
    "def visualize_policy(agent, env, trajectory=None):\n",
    "    \"\"\"\n",
    "    Visualizes the learned policy of an agent in a given environment.\n",
    "\n",
    "    Parameters:\n",
    "    - agent: The trained RL agent with Q-values and greedy policy.\n",
    "    - env: The environment object, which should contain a MAP attribute and action mapping.\n",
    "    - trajectory: A list of visited (state_x, state_y) pairs, showing an example path (optional).\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(20, 16), dpi=300)\n",
    "\n",
    "    # Display environment map\n",
    "    ax.imshow(env.unwrapped.MAP, cmap=\"gray\")\n",
    "\n",
    "    # Draw finish line (goal area)\n",
    "    plt.plot(np.full(fill_value=99, shape=31), np.arange(31), color=\"#32CD32\", linewidth=10)\n",
    "\n",
    "    # Print max Q-value for a sample state\n",
    "    print(f\"Max Q-value at (15,0): {np.max(agent.Q[15, 0, :])}\")\n",
    "\n",
    "    # If a trajectory is provided, plot the agent's path\n",
    "    if trajectory:\n",
    "        path_x = [x for _, x in trajectory]\n",
    "        path_y = [y for y, _ in trajectory]\n",
    "        ax.plot(path_x, path_y, color=\"blue\", linewidth=2)\n",
    "\n",
    "    # Draw policy arrows (greedy actions)\n",
    "    for i in range(31):\n",
    "        for j in range(99):\n",
    "            action = agent.greedy_action((i, j))  # Get best action\n",
    "            dy, dx = env.unwrapped.action_to_direction[action]  # Convert action to movement direction\n",
    "            ax.arrow(j - dx * 0.25, i - dy * 0.25, dx * 0.25, dy * 0.25, \n",
    "                     head_width=0.25, head_length=0.25, color=\"red\")\n",
    "\n",
    "    plt.title(\"Learned Policy Visualization\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay(time:int, start_value:float, n_episodes:int,A:float=0.8,B:float=0.3,C:float=0.2, lower_bound:float = 0.01):\n",
    "        standardized_time=(time-A*n_episodes)/(B*n_episodes)\n",
    "        csh = np.cosh(np.exp(-standardized_time))\n",
    "        epsilon=1.1-(1/csh+(time*C/n_episodes))\n",
    "        return max(epsilon*start_value, lower_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20 bins for position and velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 5000/5000 [02:44<00:00, 30.35it/s, steps=1000, epsilon=1, Running Return=-1e+3]\n"
     ]
    }
   ],
   "source": [
    "# Create Q-learning agent (off-policy)\n",
    "agent_q = TD0Agent(\n",
    "    action_dim=3,          \n",
    "    epsilon=1.0,           \n",
    "    alpha=0.1,             \n",
    "    gamma=0.99,            \n",
    "    offpolicy=True,        \n",
    "    n_bins_position=20,    \n",
    "    n_bins_velocity=20     \n",
    ")\n",
    "\n",
    "# Train the Q-learning agent\n",
    "train_agent(agent_q, env, total_episodes=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating video...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kasgr\\AppData\\Roaming\\Python\\Python313\\site-packages\\gymnasium\\wrappers\\rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\kasgr\\OneDrive\\Dokumenter\\Master i Informatikk\\INF266\\inf266mandatory4\\videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"videos\\agent_q-episode-0.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trajectory_agent_q = get_trajectory(agent_q, env, \"agent_q\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 5000/5000 [03:15<00:00, 25.61it/s, steps=1000, epsilon=1, Running Return=-1e+3]\n"
     ]
    }
   ],
   "source": [
    "# Create SARSA agent (on-policy)\n",
    "agent_sarsa = TD0Agent(\n",
    "    action_dim=3,          \n",
    "    epsilon=1.0,           \n",
    "    alpha=0.1,             \n",
    "    gamma=0.99,            \n",
    "    offpolicy=False,       \n",
    "    n_bins_position=30,    \n",
    "    n_bins_velocity=30     \n",
    ")\n",
    "\n",
    "# Train the SARSA agent\n",
    "train_agent(agent_sarsa, env, total_episodes=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kasgr\\AppData\\Roaming\\Python\\Python313\\site-packages\\gymnasium\\wrappers\\rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\kasgr\\OneDrive\\Dokumenter\\Master i Informatikk\\INF266\\inf266mandatory4\\videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating video...\n",
      "Video saved.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"videos\\agent_sarsa-episode-0.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trajectory_sarsa = get_trajectory(agent_sarsa, env, \"agent_sarsa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  18%|█▊        | 1811/10000 [02:04<09:03, 15.06it/s, steps=1000, epsilon=1, Running Return=-1e+3]"
     ]
    }
   ],
   "source": [
    "# Create N-Step SARSA agent\n",
    "agent_nstep_sarsa = NstepSarsaAgent(\n",
    "    action_dim=3,          \n",
    "    epsilon=1.0,           \n",
    "    alpha=0.1,             \n",
    "    gamma=0.99,            \n",
    "    n=3,                   \n",
    "    n_bins_position=20,    \n",
    "    n_bins_velocity=20     \n",
    ")\n",
    "\n",
    "# Train the N-step SARSA agent\n",
    "train_agent(agent_nstep_sarsa, env, total_episodes=10_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_nstep_sarsa = get_trajectory(agent_nstep_sarsa, env, \"agent_nstep_sarsa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30 bins for position and 30 bins for velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Q-learning agent (off-policy)\n",
    "agent_q2 = TD0Agent(\n",
    "    action_dim=3,          \n",
    "    epsilon=1.0,           \n",
    "    alpha=0.1,             \n",
    "    gamma=0.99,            \n",
    "    offpolicy=True,        \n",
    "    n_bins_position=30,    \n",
    "    n_bins_velocity=30     \n",
    ")\n",
    "\n",
    "# Train the Q-learning agent\n",
    "train_agent(agent_q2, env, total_episodes=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_agent_q2 = get_trajectory(agent_q2, env, \"agent_q2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SARSA agent (on-policy)\n",
    "agent_sarsa2 = TD0Agent(\n",
    "    action_dim=3,          \n",
    "    epsilon=1.0,           \n",
    "    alpha=0.1,             \n",
    "    gamma=0.99,            \n",
    "    offpolicy=False,       \n",
    "    n_bins_position=30,    \n",
    "    n_bins_velocity=30     \n",
    ")\n",
    "\n",
    "# Train the SARSA agent\n",
    "train_agent(agent_sarsa, env, total_episodes=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_sarsa2 = get_trajectory(agent_sarsa2, env, \"agent_sarsa2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create N-Step SARSA agent\n",
    "agent_nstep_sarsa2 = NstepSarsaAgent(\n",
    "    action_dim=3,          \n",
    "    epsilon=1.0,           \n",
    "    alpha=0.1,             \n",
    "    gamma=0.99,            \n",
    "    n=3,                   \n",
    "    n_bins_position=30,    \n",
    "    n_bins_velocity=30     \n",
    ")\n",
    "\n",
    "# Train the N-step SARSA agent\n",
    "train_agent(agent_nstep_sarsa, env, total_episodes=10_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_nstep_sarsa2 = get_trajectory(agent_nstep_sarsa2, env, \"agent_nstep_sarsa2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INF266",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
