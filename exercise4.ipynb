{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "import imageio\n",
    "import glob\n",
    "import pickle\n",
    "import IPython\n",
    "import os\n",
    "from typing import Tuple, List\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0', max_episode_steps = 1000, render_mode=\"rgb_array\")\n",
    "env.action_space.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(3)\n",
      "Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
      "[-0.43395966  0.        ]\n",
      "{}\n",
      "[-4.3362388e-01  3.3578029e-04] -1.0 False False {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:488: RuntimeWarning: Your system is avx2 capable but pygame was not built with support for it. The performance of some of your blits could be adversely affected. Consider enabling compile time detection with environment variables like PYGAME_DETECT_AVX2=1 if you are compiling without cross compilation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "#obs : position, velocity\n",
    "obs, info = env.reset() # initial observation\n",
    "print(obs)  \n",
    "print(info)  \n",
    "# take a step\n",
    "action = 2  # Push right\n",
    "obs, reward, terminated, truncated, info = env.step(action)\n",
    "print(obs, reward, terminated, truncated, info)\n",
    "#rendering\n",
    "frame = env.render()\n",
    "print(type(frame))  # <class 'numpy.ndarray'>\n",
    "plt.imshow(frame)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state space ranges from -1.2 to 0.6 for position and -0.07 to 0.07 for velocity\n",
    "# action space is discrete with 3 possible actions\n",
    "# reward is -1 for each time step until the goal position of 0.5 is reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_state(state, agent):\n",
    "    position, velocity = state\n",
    "    \n",
    "    # Create bins dynamically based on agent's n_bins\n",
    "    position_bins = np.linspace(-1.2, 0.6, agent.n_bins_position)\n",
    "    velocity_bins = np.linspace(-0.07, 0.07, agent.n_bins_velocity)\n",
    "\n",
    "    position_index = np.digitize(position, position_bins) - 1\n",
    "    velocity_index = np.digitize(velocity, velocity_bins) - 1\n",
    "\n",
    "    return (position_index, velocity_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Q table\n",
    "def initialize_q(n_bins_position, n_bins_velocity, n_actions):\n",
    "    n_actions = env.action_space.n\n",
    "    n_states = (n_bins_position, n_bins_velocity)\n",
    "    Q = np.zeros(n_states + (n_actions,))\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all the agents\n",
    "class BaseAgent(ABC):\n",
    "    def __init__(self, action_dim:int, epsilon: float, gamma: float = 1.0,n_bins_position:int=20,n_bins_velocity:int=20):\n",
    "        self.action_dim = action_dim\n",
    "        self.epsilon = self.initial_epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.n_bins_position = n_bins_position\n",
    "        self.n_bins_velocity = n_bins_velocity\n",
    "        self.Q = np.zeros((self.n_bins_position, self.n_bins_velocity, self.action_dim))  \n",
    "        \n",
    "\n",
    "    @abstractmethod\n",
    "    def select_action(self, state) -> int:\n",
    "        \"\"\"Select an action given the state\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def learn(self, *args, **kwargs) -> None:\n",
    "        \"\"\"Update Q-values based on learning method\"\"\"\n",
    "        pass\n",
    "    \n",
    "\n",
    "    def select_action(self, state) -> int:\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            return np.random.choice(self.action_dim)\n",
    "        else:\n",
    "            return self.greedy_action(state)\n",
    "\n",
    "    def greedy_action(self, state) -> int:\n",
    "        action_values = self.Q[state[0], state[1], :]\n",
    "        return np.argmax(action_values)  # Choose the best action\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.Q = np.zeros((31, 100, self.action_dim))  # Reset Q-table\n",
    "        self.epsilon = self.initial_epsilon\n",
    "\n",
    "    def __getstate__(self):\n",
    "        return self.__dict__.copy()\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        self.__dict__.update(state)\n",
    "\n",
    "\n",
    "class TD0Agent(BaseAgent):  \n",
    "    def __init__(self, action_dim, epsilon, alpha, gamma=1, offpolicy=False, n_bins_position=20, n_bins_velocity=20):\n",
    "        super().__init__(action_dim, epsilon, gamma, n_bins_position, n_bins_velocity)\n",
    "        self.offpolicy = offpolicy\n",
    "        self.alpha = self.initial_alpha = alpha\n",
    "\n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.alpha = self.initial_alpha\n",
    "\n",
    "        \n",
    "    #def learn(self, experience:Experience)->None:\n",
    "    #    state, action, reward, next_state, done = experience\n",
    "    #    if self.offpolicy:\n",
    "    #        next_action = self.greedy_action(next_state) # q learning\n",
    "    #    else:\n",
    "    #        next_action = self.select_action(next_state) # sarsa\n",
    "\n",
    "    #    next_max = self.Q[next_state[0],next_state[1], next_action] \n",
    "    \n",
    "    #    self.Q[state][action] = self.Q[state][action] + self.alpha*(reward+  self.gamma*next_max - self.Q[state][action] )\n",
    "    def learn(self, experience: Experience) -> None:\n",
    "        state, action, reward, next_state, done = experience\n",
    "        if self.offpolicy:\n",
    "            next_action = self.greedy_action(next_state)  # Q-learning\n",
    "        else:\n",
    "            next_action = self.select_action(next_state)  # SARSA\n",
    "\n",
    "        next_max = self.Q[next_state[0], next_state[1], next_action] \n",
    "\n",
    "        # Fix: Use correct Q-table indexing\n",
    "        self.Q[state[0], state[1], action] += self.alpha * (\n",
    "            reward + self.gamma * next_max - self.Q[state[0], state[1], action]\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class NstepSarsaAgent(BaseAgent):\n",
    "    def __init__(self, action_dim, epsilon, alpha, gamma=1, n=3, n_bins_position=20, n_bins_velocity=20):\n",
    "        super().__init__(action_dim, epsilon, gamma, n_bins_position, n_bins_velocity)\n",
    "        self.alpha = self.initial_alpha = alpha\n",
    "        self.n = n  # Number of steps\n",
    "        self.memory = deque(maxlen=n+1)  # Store (state, action, reward)\n",
    "\n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.alpha = self.initial_alpha\n",
    "        self.memory.clear()\n",
    "\n",
    "    def learn(self, experience:Experience):\n",
    "        \"\"\"\n",
    "        Stores the current experience and updates Q-values when enough steps are collected.\n",
    "        \"\"\"\n",
    "        _, _, _, _, done = experience\n",
    "        self.memory.append(experience)\n",
    "\n",
    "       \n",
    "\n",
    "        if len(self.memory) < (self.n+1) and not done:  # if not reach terminal state: wait until enough steps are collected'\n",
    "            return\n",
    "\n",
    "        experiences = self.memory.copy()\n",
    "        while len(experiences) > 0:\n",
    "            G = 0 # Return\n",
    "            for i, (_, _, r, _,_) in enumerate(experiences):\n",
    "                G += (self.gamma ** (i+1)) * r  # Compute n-step return\n",
    "           \n",
    "            last_experience = experiences[-1]  # Last step\n",
    "            next_state = last_experience.next_state\n",
    "            next_action = self.select_action(next_state)\n",
    "            Q_n = self.Q[next_state[0], next_state[1], next_action]\n",
    "            G += (self.gamma ** self.n) * Q_n\n",
    "           \n",
    "            state, action, _, _, _ = experiences[0]\n",
    "            td_error = G - self.Q[state[0], state[1], action] # Temporal difference error r_1*gamma^0 + r_2*gamma^1 + ... + r_n*gamma^(n-1) + Q(S_n, A_n) - Q(S_0, A_0)\n",
    "            self.Q[state[0], state[1], action] += self.alpha * td_error\n",
    "            experiences.popleft()\n",
    "        \n",
    "        if len(self.memory) >= self.n + 1:\n",
    "            self.memory.popleft()\n",
    "        if done:\n",
    "            self.memory.clear()          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training loop\n",
    "def train_agent(agent, env, total_episodes=10000, decay_fn=None):\n",
    "    \"\"\"\n",
    "    Trains an agent (either MC or TD0) in MountainCar-v0.\n",
    "    \"\"\"\n",
    "    time_outs = 0\n",
    "    env.action_space.seed(42)\n",
    "\n",
    "    with tqdm(total=total_episodes, desc=\"Training Progress\") as pbar:\n",
    "        for episode in range(total_episodes):\n",
    "            state, _ = env.reset()\n",
    "            state = discretize_state(state, agent)  # Ensure it's discrete\n",
    "\n",
    "            done = False\n",
    "            step = 0\n",
    "            running_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                step += 1\n",
    "                action = agent.select_action(state)\n",
    "\n",
    "                # Take action\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                next_state = discretize_state(next_state,agent)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                running_reward += reward\n",
    "                experience = Experience(state, action, reward, next_state, done)\n",
    "                agent.learn(experience)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            # Apply decay to epsilon & alpha\n",
    "            if decay_fn:\n",
    "            #    agent.epsilon = decay_fn(episode, agent.initial_epsilon, total_episodes, A=0.5, B=0.2, C=0.001)\n",
    "            #    if isinstance(agent, (TD0Agent, NstepSarsaAgent)):\n",
    "            #        agent.alpha = decay_fn(episode, agent.initial_alpha, total_episodes, A=0.5, B=0.2, C=0.00001)\n",
    "                agent.epsilon = decay_fn(episode, agent.initial_epsilon, total_episodes)\n",
    "\n",
    "\n",
    "            if episode % 100 == 0:\n",
    "                state_check = (15, 10)  # Example state\n",
    "                print(f\"Q-values at {state_check}: {agent.Q[state_check]}\")\n",
    "                best_action = np.argmax(agent.Q[state_check])\n",
    "\n",
    "                print(f\"Preferred action at {state_check}: {best_action}\")  # Should become 2\n",
    "                action_counts = [0, 0, 0]  # Left, stay, right\n",
    "                for _ in range(100):\n",
    "                    test_state, _ = env.reset()\n",
    "                    test_state = discretize_state(test_state, agent)\n",
    "                    action = agent.greedy_action(test_state)\n",
    "                    action_counts[action] += 1\n",
    "                print(f\"Episode {episode} Action Distribution: {action_counts}\")\n",
    "\n",
    "            # Logging\n",
    "            pbar.set_postfix({\"steps\": step, \"epsilon\": agent.epsilon, \"Running Return\": running_reward})\n",
    "            pbar.update(1)\n",
    "\n",
    "\n",
    "\n",
    "def get_trajectory(agent:BaseAgent,env:gym.Env, filename:str):\n",
    "    print(\"Creating video...\")\n",
    "    video_folder = \"videos\"\n",
    "    if not os.path.exists(video_folder):\n",
    "        os.makedirs(video_folder)\n",
    "    trajectory = []\n",
    "    env = gym.wrappers.RecordVideo(env, video_folder=video_folder, name_prefix=filename)\n",
    "    observation, info = env.reset()\n",
    "    observation = discretize_state(observation,agent)\n",
    "\n",
    "\n",
    "    while True:\n",
    "        env.render()\n",
    "        state = discretize_state(observation,agent)\n",
    "        trajectory.append(state)\n",
    "        action = agent.greedy_action(state)\n",
    "\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        observation = discretize_state(observation,agent)\n",
    "        if terminated or truncated:\n",
    "            env.render()\n",
    "            break\n",
    "    \n",
    "    state = discretize_state(observation,agent)\n",
    "    trajectory.append(state)\n",
    "    env.close()\n",
    "    print(\"Video saved.\")\n",
    "    video_files = sorted(\n",
    "        [f for f in os.listdir(video_folder) if f.endswith(\".mp4\") and f.startswith(filename)],\n",
    "        key=lambda f: os.path.getmtime(os.path.join(video_folder, f)),  # Sort by creation time\n",
    "        reverse=True\n",
    "    )\n",
    "    video_path = os.path.join(video_folder, video_files[0])\n",
    "    IPython.display.display(IPython.display.Video(video_path))\n",
    "    return trajectory\n",
    "\n",
    "\n",
    "\n",
    "def visualize_policy(agent, env, trajectory=None):\n",
    "    \"\"\"\n",
    "    Visualizes the learned policy of an agent in a given environment.\n",
    "\n",
    "    Parameters:\n",
    "    - agent: The trained RL agent with Q-values and greedy policy.\n",
    "    - env: The environment object, which should contain a MAP attribute and action mapping.\n",
    "    - trajectory: A list of visited (state_x, state_y) pairs, showing an example path (optional).\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(20, 16), dpi=300)\n",
    "\n",
    "    # Display environment map\n",
    "    ax.imshow(env.unwrapped.MAP, cmap=\"gray\")\n",
    "\n",
    "    # Draw finish line (goal area)\n",
    "    plt.plot(np.full(fill_value=99, shape=31), np.arange(31), color=\"#32CD32\", linewidth=10)\n",
    "\n",
    "    # Print max Q-value for a sample state\n",
    "    print(f\"Max Q-value at (15,0): {np.max(agent.Q[15, 0, :])}\")\n",
    "\n",
    "    # If a trajectory is provided, plot the agent's path\n",
    "    if trajectory:\n",
    "        path_x = [x for _, x in trajectory]\n",
    "        path_y = [y for y, _ in trajectory]\n",
    "        ax.plot(path_x, path_y, color=\"blue\", linewidth=2)\n",
    "\n",
    "    # Draw policy arrows (greedy actions)\n",
    "    for i in range(31):\n",
    "        for j in range(99):\n",
    "            action = agent.greedy_action((i, j))  # Get best action\n",
    "            dy, dx = env.unwrapped.action_to_direction[action]  # Convert action to movement direction\n",
    "            ax.arrow(j - dx * 0.25, i - dy * 0.25, dx * 0.25, dy * 0.25, \n",
    "                     head_width=0.25, head_length=0.25, color=\"red\")\n",
    "\n",
    "    plt.title(\"Learned Policy Visualization\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay(time:int, start_value:float, n_episodes:int,A:float=0.8,B:float=0.3,C:float=0.2, lower_bound:float = 0.01):\n",
    "        standardized_time=(time-A*n_episodes)/(B*n_episodes)\n",
    "        csh = np.cosh(np.exp(-standardized_time))\n",
    "        epsilon=1.1-(1/csh+(time*C/n_episodes))\n",
    "        return max(epsilon*start_value, lower_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decay 2\n",
    "def decay_epsilon(episode, initial_epsilon=1.0, total_episodes=10000, min_epsilon=0.01):\n",
    "    return max(initial_epsilon * (0.999 ** episode), min_epsilon)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train 2\n",
    "# Define the training loop with suggested parameters from hælfi with seperate rendering environment\n",
    "env2 = gym.make('MountainCar-v0', max_episode_steps = 1000)\n",
    "env2.action_space.seed(42)\n",
    "def train(agent, env, total_episodes=10000):\n",
    "    \"\"\"\n",
    "    Trains an agent in MountainCar-v0 without rendering (to improve speed).\n",
    "    \"\"\"\n",
    "    env.action_space.seed(42)\n",
    "\n",
    "    with tqdm(total=total_episodes, desc=\"Training Progress\") as pbar:\n",
    "        for episode in range(total_episodes):\n",
    "            state, _ = env.reset()\n",
    "            state = discretize_state(state, agent)\n",
    "\n",
    "            done = False\n",
    "            step = 0\n",
    "            running_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                step += 1\n",
    "                action = agent.select_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                next_state = discretize_state(next_state, agent)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                # Store experience and update Q-table\n",
    "                experience = Experience(state, action, reward, next_state, done)\n",
    "                agent.learn(experience)\n",
    "                state = next_state\n",
    "\n",
    "            if episode % 1000 == 0:\n",
    "                state_check = (15, 10)  # Example state\n",
    "                best_action = np.argmax(agent.Q[state_check])\n",
    "                print(f\"Q-values at {state_check}: {agent.Q[state_check]}\")\n",
    "                print(f\"Preferred action at {state_check}: {best_action}\")  # Should become 2\n",
    "\n",
    "\n",
    "            # Decay epsilon\n",
    "            agent.epsilon = max(agent.epsilon * 0.995, 0.01)  # Ensure epsilon does not go below 0.01\n",
    "\n",
    "            # Logging progress\n",
    "            pbar.set_postfix({\"steps\": step, \"epsilon\": agent.epsilon, \"Running Return\": running_reward})\n",
    "            pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Q-learning agent with suggested parameters\n",
    "agent_q = TD0Agent(\n",
    "    action_dim=3,          \n",
    "    epsilon=1.0,           # Start with full exploration\n",
    "    alpha=0.1,             # Learning rate\n",
    "    gamma=0.9,             # Slightly lower discount factor\n",
    "    offpolicy=True,        \n",
    "    n_bins_position=40,    \n",
    "    n_bins_velocity=40     \n",
    ")\n",
    "\n",
    "# Train the agent\n",
    "train(agent_q, env2, total_episodes=10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a separate environment for rendering\n",
    "render_env = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\n",
    "\n",
    "# Visualize the learned policy\n",
    "trajectory = get_trajectory(agent_q, render_env, \"trained_agent_render\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20 bins for position and velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 4/5000 [00:00<03:45, 22.18it/s, steps=1000, epsilon=0.996, Running Return=-1e+3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-0.3 -0.3  0. ]\n",
      "Preferred action at (15, 10): 2\n",
      "Episode 0 Action Distribution: [22, 33, 45]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 104/5000 [00:05<04:15, 19.18it/s, steps=1000, epsilon=0.902, Running Return=-1e+3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-5.59476202 -5.67526257 -5.79586072]\n",
      "Preferred action at (15, 10): 0\n",
      "Episode 100 Action Distribution: [57, 0, 43]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   4%|▍         | 203/5000 [00:10<04:05, 19.58it/s, steps=1000, epsilon=0.816, Running Return=-1e+3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.26740595 -6.31233037 -6.28442595]\n",
      "Preferred action at (15, 10): 0\n",
      "Episode 200 Action Distribution: [24, 0, 76]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   6%|▌         | 305/5000 [00:15<03:26, 22.72it/s, steps=976, epsilon=0.738, Running Return=-976]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.54639724 -6.53961702 -6.56109572]\n",
      "Preferred action at (15, 10): 1\n",
      "Episode 300 Action Distribution: [25, 0, 75]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   8%|▊         | 407/5000 [00:19<02:37, 29.21it/s, steps=514, epsilon=0.666, Running Return=-514]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.61254479 -6.61676286 -6.63809598]\n",
      "Preferred action at (15, 10): 0\n",
      "Episode 400 Action Distribution: [62, 10, 28]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  10%|█         | 506/5000 [00:22<02:32, 29.56it/s, steps=352, epsilon=0.603, Running Return=-352]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.64136297 -6.64705976 -6.64786157]\n",
      "Preferred action at (15, 10): 0\n",
      "Episode 500 Action Distribution: [86, 0, 14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  12%|█▏        | 607/5000 [00:26<02:20, 31.34it/s, steps=338, epsilon=0.545, Running Return=-338]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.65362801 -6.6544158  -6.65658407]\n",
      "Preferred action at (15, 10): 0\n",
      "Episode 600 Action Distribution: [64, 0, 36]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  14%|█▍        | 708/5000 [00:28<01:40, 42.71it/s, steps=646, epsilon=0.492, Running Return=-646]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.65945856 -6.65926825 -6.65922733]\n",
      "Preferred action at (15, 10): 2\n",
      "Episode 700 Action Distribution: [60, 0, 40]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  16%|█▌        | 811/5000 [00:30<01:15, 55.33it/s, steps=288, epsilon=0.444, Running Return=-288]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.65991705 -6.66248912 -6.66270436]\n",
      "Preferred action at (15, 10): 0\n",
      "Episode 800 Action Distribution: [56, 9, 35]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  18%|█▊        | 912/5000 [00:32<01:08, 59.62it/s, steps=252, epsilon=0.402, Running Return=-252]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66360246 -6.66352394 -6.66371806]\n",
      "Preferred action at (15, 10): 1\n",
      "Episode 900 Action Distribution: [56, 0, 44]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  20%|██        | 1013/5000 [00:34<01:00, 66.03it/s, steps=350, epsilon=0.363, Running Return=-350]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66367661 -6.66340711 -6.66366738]\n",
      "Preferred action at (15, 10): 1\n",
      "Episode 1000 Action Distribution: [57, 12, 31]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  22%|██▏       | 1113/5000 [00:35<00:58, 66.04it/s, steps=248, epsilon=0.328, Running Return=-248]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.6640754  -6.6638001  -6.66383092]\n",
      "Preferred action at (15, 10): 1\n",
      "Episode 1100 Action Distribution: [90, 0, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  24%|██▍       | 1212/5000 [00:37<01:00, 62.21it/s, steps=274, epsilon=0.297, Running Return=-274]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66469135 -6.66369319 -6.66278087]\n",
      "Preferred action at (15, 10): 2\n",
      "Episode 1200 Action Distribution: [89, 0, 11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  26%|██▌       | 1311/5000 [00:38<00:57, 64.06it/s, steps=269, epsilon=0.269, Running Return=-269]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66471685 -6.66466022 -6.66468661]\n",
      "Preferred action at (15, 10): 1\n",
      "Episode 1300 Action Distribution: [58, 0, 42]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  28%|██▊       | 1413/5000 [00:40<00:53, 67.63it/s, steps=732, epsilon=0.243, Running Return=-732]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66449521 -6.66538647 -6.66526881]\n",
      "Preferred action at (15, 10): 0\n",
      "Episode 1400 Action Distribution: [58, 0, 42]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  30%|███       | 1515/5000 [00:41<00:42, 81.69it/s, steps=294, epsilon=0.22, Running Return=-294] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66507933 -6.6655402  -6.66557579]\n",
      "Preferred action at (15, 10): 0\n",
      "Episode 1500 Action Distribution: [66, 0, 34]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  32%|███▏      | 1611/5000 [00:43<00:51, 66.07it/s, steps=368, epsilon=0.2, Running Return=-368]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66558417 -6.66571856 -6.66571997]\n",
      "Preferred action at (15, 10): 0\n",
      "Episode 1600 Action Distribution: [90, 0, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  34%|███▍      | 1708/5000 [00:44<00:53, 62.11it/s, steps=1000, epsilon=0.181, Running Return=-1e+3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.6660513  -6.66612198 -6.66608585]\n",
      "Preferred action at (15, 10): 0\n",
      "Episode 1700 Action Distribution: [53, 0, 47]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  36%|███▋      | 1813/5000 [00:46<00:50, 63.15it/s, steps=312, epsilon=0.163, Running Return=-312]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66592136 -6.66596042 -6.66607643]\n",
      "Preferred action at (15, 10): 0\n",
      "Episode 1800 Action Distribution: [53, 16, 31]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  38%|███▊      | 1916/5000 [00:47<00:39, 77.32it/s, steps=322, epsilon=0.147, Running Return=-322]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66614726 -6.66626644 -6.6662476 ]\n",
      "Preferred action at (15, 10): 0\n",
      "Episode 1900 Action Distribution: [91, 0, 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  40%|████      | 2015/5000 [00:49<00:41, 71.43it/s, steps=226, epsilon=0.133, Running Return=-226]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66622878 -6.66622805 -6.6662376 ]\n",
      "Preferred action at (15, 10): 1\n",
      "Episode 2000 Action Distribution: [57, 0, 43]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  42%|████▏     | 2113/5000 [00:50<00:39, 73.03it/s, steps=453, epsilon=0.121, Running Return=-453]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66622124 -6.66635092 -6.66633968]\n",
      "Preferred action at (15, 10): 0\n",
      "Episode 2100 Action Distribution: [61, 0, 39]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  44%|████▍     | 2217/5000 [00:51<00:34, 81.15it/s, steps=241, epsilon=0.109, Running Return=-241]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66638586 -6.66639337 -6.66639992]\n",
      "Preferred action at (15, 10): 0\n",
      "Episode 2200 Action Distribution: [62, 0, 38]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  46%|████▋     | 2317/5000 [00:52<00:28, 93.27it/s, steps=228, epsilon=0.0985, Running Return=-228]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66640157 -6.66641786 -6.66639992]\n",
      "Preferred action at (15, 10): 2\n",
      "Episode 2300 Action Distribution: [66, 0, 34]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  48%|████▊     | 2416/5000 [00:54<00:30, 85.95it/s, steps=188, epsilon=0.0892, Running Return=-188]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66647772 -6.66647219 -6.66646649]\n",
      "Preferred action at (15, 10): 2\n",
      "Episode 2400 Action Distribution: [60, 0, 40]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  50%|█████     | 2516/5000 [00:55<00:32, 76.11it/s, steps=163, epsilon=0.0808, Running Return=-163]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66647772 -6.66639355 -6.66648203]\n",
      "Preferred action at (15, 10): 1\n",
      "Episode 2500 Action Distribution: [61, 0, 39]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  52%|█████▏    | 2617/5000 [00:56<00:24, 98.03it/s, steps=251, epsilon=0.0729, Running Return=-251] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66651119 -6.66651414 -6.66650507]\n",
      "Preferred action at (15, 10): 2\n",
      "Episode 2600 Action Distribution: [30, 35, 35]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  54%|█████▍    | 2719/5000 [00:57<00:35, 63.95it/s, steps=254, epsilon=0.0659, Running Return=-254]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66646766 -6.66651414 -6.6665149 ]\n",
      "Preferred action at (15, 10): 0\n",
      "Episode 2700 Action Distribution: [53, 11, 36]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  56%|█████▋    | 2814/5000 [00:59<00:29, 75.29it/s, steps=371, epsilon=0.0599, Running Return=-371]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66620783 -6.66651414 -6.66651436]\n",
      "Preferred action at (15, 10): 0\n",
      "Episode 2800 Action Distribution: [66, 0, 34]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  58%|█████▊    | 2918/5000 [01:00<00:23, 88.64it/s, steps=181, epsilon=0.054, Running Return=-181] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66649586 -6.66651092 -6.66650964]\n",
      "Preferred action at (15, 10): 0\n",
      "Episode 2900 Action Distribution: [54, 9, 37]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  60%|██████    | 3019/5000 [01:01<00:20, 95.11it/s, steps=169, epsilon=0.0488, Running Return=-169]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.6664596  -6.66651092 -6.66650964]\n",
      "Preferred action at (15, 10): 0\n",
      "Episode 3000 Action Distribution: [18, 0, 82]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  62%|██████▏   | 3121/5000 [01:02<00:18, 101.90it/s, steps=153, epsilon=0.044, Running Return=-153] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66652163 -6.66654142 -6.66653706]\n",
      "Preferred action at (15, 10): 0\n",
      "Episode 3100 Action Distribution: [92, 0, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  64%|██████▍   | 3220/5000 [01:03<00:18, 96.48it/s, steps=269, epsilon=0.0399, Running Return=-269] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66655654 -6.66656145 -6.66656471]\n",
      "Preferred action at (15, 10): 0\n",
      "Episode 3200 Action Distribution: [30, 37, 33]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  66%|██████▋   | 3322/5000 [01:04<00:15, 107.57it/s, steps=218, epsilon=0.036, Running Return=-218] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66650448 -6.66656165 -6.66655817]\n",
      "Preferred action at (15, 10): 0\n",
      "Episode 3300 Action Distribution: [92, 0, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  68%|██████▊   | 3414/5000 [01:05<00:19, 81.30it/s, steps=530, epsilon=0.0329, Running Return=-530] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66650966 -6.66656165 -6.66655817]\n",
      "Preferred action at (15, 10): 0\n",
      "Episode 3400 Action Distribution: [29, 28, 43]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  70%|███████   | 3517/5000 [01:06<00:15, 93.64it/s, steps=200, epsilon=0.0296, Running Return=-200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66651517 -6.66656165 -6.66655817]\n",
      "Preferred action at (15, 10): 0\n",
      "Episode 3500 Action Distribution: [59, 0, 41]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  72%|███████▏  | 3617/5000 [01:07<00:16, 84.33it/s, steps=194, epsilon=0.0268, Running Return=-194]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66654987 -6.66656375 -6.66656315]\n",
      "Preferred action at (15, 10): 0\n",
      "Episode 3600 Action Distribution: [59, 0, 41]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  74%|███████▍  | 3714/5000 [01:08<00:15, 84.07it/s, steps=223, epsilon=0.0243, Running Return=-223]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66656009 -6.66656375 -6.66656315]\n",
      "Preferred action at (15, 10): 0\n",
      "Episode 3700 Action Distribution: [26, 29, 45]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  76%|███████▋  | 3817/5000 [01:10<00:12, 92.30it/s, steps=251, epsilon=0.022, Running Return=-251] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66657682 -6.66658762 -6.66658721]\n",
      "Preferred action at (15, 10): 0\n",
      "Episode 3800 Action Distribution: [61, 0, 39]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  78%|███████▊  | 3919/5000 [01:11<00:11, 95.29it/s, steps=193, epsilon=0.0198, Running Return=-193]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66658963 -6.66658762 -6.6665926 ]\n",
      "Preferred action at (15, 10): 1\n",
      "Episode 3900 Action Distribution: [61, 0, 39]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  80%|████████  | 4019/5000 [01:12<00:09, 101.46it/s, steps=310, epsilon=0.0179, Running Return=-310]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66659748 -6.66659118 -6.6665926 ]\n",
      "Preferred action at (15, 10): 1\n",
      "Episode 4000 Action Distribution: [75, 0, 25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  82%|████████▏ | 4117/5000 [01:13<00:10, 87.36it/s, steps=195, epsilon=0.0163, Running Return=-195] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66660723 -6.66660742 -6.66660846]\n",
      "Preferred action at (15, 10): 0\n",
      "Episode 4100 Action Distribution: [29, 29, 42]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  84%|████████▍ | 4222/5000 [01:14<00:07, 105.81it/s, steps=169, epsilon=0.0146, Running Return=-169]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66659983 -6.66660742 -6.66660846]\n",
      "Preferred action at (15, 10): 0\n",
      "Episode 4200 Action Distribution: [28, 28, 44]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  86%|████████▋ | 4319/5000 [01:15<00:07, 88.05it/s, steps=191, epsilon=0.0133, Running Return=-191] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66662232 -6.66662466 -6.66662521]\n",
      "Preferred action at (15, 10): 0\n",
      "Episode 4300 Action Distribution: [56, 0, 44]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  88%|████████▊ | 4416/5000 [01:16<00:07, 82.31it/s, steps=191, epsilon=0.0121, Running Return=-191]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66663129 -6.66663391 -6.66663383]\n",
      "Preferred action at (15, 10): 0\n",
      "Episode 4400 Action Distribution: [31, 0, 69]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  90%|█████████ | 4520/5000 [01:17<00:05, 93.66it/s, steps=228, epsilon=0.0109, Running Return=-228]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66664017 -6.66663926 -6.66663783]\n",
      "Preferred action at (15, 10): 2\n",
      "Episode 4500 Action Distribution: [58, 0, 42]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  92%|█████████▏| 4621/5000 [01:18<00:03, 104.53it/s, steps=152, epsilon=0.01, Running Return=-152]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66664224 -6.66664246 -6.66663733]\n",
      "Preferred action at (15, 10): 2\n",
      "Episode 4600 Action Distribution: [58, 0, 42]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  94%|█████████▍| 4723/5000 [01:19<00:02, 99.34it/s, steps=175, epsilon=0.01, Running Return=-175] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66660252 -6.66664246 -6.66664279]\n",
      "Preferred action at (15, 10): 0\n",
      "Episode 4700 Action Distribution: [27, 0, 73]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  96%|█████████▋| 4821/5000 [01:20<00:01, 111.08it/s, steps=157, epsilon=0.01, Running Return=-157]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66661975 -6.66664246 -6.66664279]\n",
      "Preferred action at (15, 10): 0\n",
      "Episode 4800 Action Distribution: [55, 0, 45]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  98%|█████████▊| 4921/5000 [01:21<00:00, 109.50it/s, steps=166, epsilon=0.01, Running Return=-166]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values at (15, 10): [-6.66661005 -6.66664246 -6.66664279]\n",
      "Preferred action at (15, 10): 0\n",
      "Episode 4900 Action Distribution: [24, 0, 76]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 5000/5000 [01:22<00:00, 60.62it/s, steps=171, epsilon=0.01, Running Return=-171] \n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0', max_episode_steps=1000, render_mode=None)\n",
    "env.action_space.seed(42)\n",
    "# Create Q-learning agent (off-policy)\n",
    "agent_q = TD0Agent(\n",
    "    action_dim=3,          \n",
    "    epsilon=1.0,           \n",
    "    alpha=0.3,  # Lower learning rate for more stable updates            \n",
    "    gamma=0.85,  # Keep gamma lowered for short-term improvements  \n",
    "    offpolicy=True,        \n",
    "    n_bins_position=30,    \n",
    "    n_bins_velocity=30     \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Train the Q-learning agent\n",
    "train_agent(agent_q, env, total_episodes=5000, decay_fn=decay_epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating video...\n",
      "Video saved.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"videos\\agent_q-episode-0.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a separate environment for rendering\n",
    "render_env = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\n",
    "\n",
    "# Generate trajectory\n",
    "trajectory_agent_q = get_trajectory(agent_q, render_env, \"agent_q\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_agent_q = get_trajectory(agent_q, env, \"agent_q\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SARSA agent (on-policy)\n",
    "agent_sarsa = TD0Agent(\n",
    "    action_dim=3,          \n",
    "    epsilon=1.0,           \n",
    "    alpha=0.1,             \n",
    "    gamma=0.99,            \n",
    "    offpolicy=False,       \n",
    "    n_bins_position=25,    \n",
    "    n_bins_velocity=25     \n",
    ")\n",
    "\n",
    "# Train the SARSA agent\n",
    "train_agent(agent_sarsa, env, total_episodes=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_sarsa = get_trajectory(agent_sarsa, env, \"agent_sarsa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create N-Step SARSA agent\n",
    "agent_nstep_sarsa = NstepSarsaAgent(\n",
    "    action_dim=3,          \n",
    "    epsilon=1.0,           \n",
    "    alpha=0.1,             \n",
    "    gamma=0.99,            \n",
    "    n=3,                   \n",
    "    n_bins_position=20,    \n",
    "    n_bins_velocity=20     \n",
    ")\n",
    "\n",
    "# Train the N-step SARSA agent\n",
    "train_agent(agent_nstep_sarsa, env, total_episodes=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_nstep_sarsa = get_trajectory(agent_nstep_sarsa, env, \"agent_nstep_sarsa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30 bins for position and 30 bins for velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Q-learning agent (off-policy)\n",
    "agent_q2 = TD0Agent(\n",
    "    action_dim=3,          \n",
    "    epsilon=1.0,           \n",
    "    alpha=0.1,             \n",
    "    gamma=0.99,            \n",
    "    offpolicy=True,        \n",
    "    n_bins_position=30,    \n",
    "    n_bins_velocity=30     \n",
    ")\n",
    "\n",
    "# Train the Q-learning agent\n",
    "train_agent(agent_q2, env, total_episodes=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_agent_q2 = get_trajectory(agent_q2, env, \"agent_q2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SARSA agent (on-policy)\n",
    "agent_sarsa2 = TD0Agent(\n",
    "    action_dim=3,          \n",
    "    epsilon=1.0,           \n",
    "    alpha=0.1,             \n",
    "    gamma=0.99,            \n",
    "    offpolicy=False,       \n",
    "    n_bins_position=30,    \n",
    "    n_bins_velocity=30     \n",
    ")\n",
    "\n",
    "# Train the SARSA agent\n",
    "train_agent(agent_sarsa, env, total_episodes=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_sarsa2 = get_trajectory(agent_sarsa2, env, \"agent_sarsa2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create N-Step SARSA agent\n",
    "agent_nstep_sarsa2 = NstepSarsaAgent(\n",
    "    action_dim=3,          \n",
    "    epsilon=1.0,           \n",
    "    alpha=0.1,             \n",
    "    gamma=0.99,            \n",
    "    n=3,                   \n",
    "    n_bins_position=30,    \n",
    "    n_bins_velocity=30     \n",
    ")\n",
    "\n",
    "# Train the N-step SARSA agent\n",
    "train_agent(agent_nstep_sarsa, env, total_episodes=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_nstep_sarsa2 = get_trajectory(agent_nstep_sarsa2, env, \"agent_nstep_sarsa2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat for evaluation (Ikke prøvd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bins in [10, 20, 30, 40, 50]:  # Test different discretizations\n",
    "    print(f\"\\nTraining with n_bins = {bins}\")\n",
    "\n",
    "    agent_q = TD0Agent(\n",
    "        action_dim=3, epsilon=1.0, alpha=0.1, gamma=0.99,\n",
    "        offpolicy=True,  # Q-learning\n",
    "        n_bins_position=bins, n_bins_velocity=bins\n",
    "    )\n",
    "    train_agent(agent_q, env, total_episodes=10_000)\n",
    "\n",
    "    agent_sarsa = TD0Agent(\n",
    "        action_dim=3, epsilon=1.0, alpha=0.1, gamma=0.99,\n",
    "        offpolicy=False,  # SARSA\n",
    "        n_bins_position=bins, n_bins_velocity=bins\n",
    "    )\n",
    "    train_agent(agent_sarsa, env, total_episodes=10_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent, label in zip([agent_q, agent_sarsa], [\"Q-learning\", \"SARSA\"]):\n",
    "    print(f\"\\nTesting {label} with bins={bins}\")\n",
    "    trajectory = get_trajectory(agent, env, f\"mountaincar_{label}_bins{bins}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other feature engineering\n",
    "# Tile encoding\n",
    "# Normalizing input\n",
    "# Reward shaping"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INF266",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
