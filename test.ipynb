{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import skyscraper\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ex5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories = np.loadtxt(\"powered_flight.txt\", dtype=int)  # Reads as integers\n",
    "trajectories = trajectories - [1, 1, 0, 0, 1, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transistion function\n",
    "\n",
    "def transition_function(trajectories):\n",
    "    transition_dict = {}\n",
    "\n",
    "    for step in trajectories:\n",
    "        state = (step[0], step[1])  # Current state (assuming 2D state)\n",
    "        action = step[2]  # Action taken\n",
    "        new_state = (step[4], step[5])  # Next state (assuming 2D state)\n",
    "\n",
    "        # Store multiple transitions per (state, action) pair\n",
    "        if (state, action) not in transition_dict:\n",
    "            transition_dict[(state, action)] = []\n",
    "        \n",
    "        transition_dict[(state, action)].append(new_state)\n",
    "\n",
    "    return transition_dict\n",
    "\n",
    "print(transition_function(trajectories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(trajecotries):\n",
    "    reward_dict = {}\n",
    "\n",
    "    for step in trajectories:\n",
    "        state = (step[0], step[1])  # Current state (assuming 2D state)\n",
    "        action = step[2]  # Action taken\n",
    "        reward = step[3]\n",
    "\n",
    "        # Store multiple transitions per (state, action) pair\n",
    "        if (state, action) not in reward_dict:\n",
    "            reward_dict[(state, action)] = []\n",
    "        \n",
    "        reward_dict[(state, action)].append(reward)\n",
    "\n",
    "    return reward_dict\n",
    "\n",
    "print(reward_function(trajectories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('skyscraper/GridWorld-v0')\n",
    "env2 = gym.make('skyscraper/GridWorld-v0').unwrapped "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset()\n",
    "print(obs)\n",
    "obs, _, _, _, _=env.step(1)\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Chat kode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransitionModel(nn.Module):\n",
    "    def __init__(self, input_dim=3):  # (x, y, a) → (x', y')\n",
    "        super().__init__()\n",
    "        map = [range(0, 32, 1), range(0, 64, 1)]\n",
    "        self.label = list(itertools.product(*map))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32*64)\n",
    "        )\n",
    "\n",
    "    def forward(self, state_action):\n",
    "        return self.fc(state_action)\n",
    "\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, input_dim=3, output_dim=1):  # (x, y, a) → r\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, state_action):\n",
    "        return self.fc(state_action)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_from_data(recorded_trajectories, transition_model, reward_model, epochs=30):\n",
    "    \"\"\"\n",
    "    Train the transition and reward models using recorded trajectories.\n",
    "    \n",
    "    Parameters:\n",
    "    - recorded_trajectories: List of (s, a, r, s´) tuples.\n",
    "    - transition_model: Neural network for predicting next state.\n",
    "    - reward_model: Neural network for predicting reward.\n",
    "    - epochs: Number of training iterations.\n",
    "    - batch_size: Number of samples per training step.\n",
    "    \"\"\"\n",
    "    loss_fn = nn.MSELoss()  # Mean Squared Error for both models\n",
    "    optimizer_T = optim.Adam(transition_model.parameters(), lr=0.001)\n",
    "    optimizer_R = optim.Adam(reward_model.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Sample a batch of experiences\n",
    "        for state_x, state_y, action, reward, next_state_x, next_state_y in recorded_trajectories:\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "            reward = torch.tensor(reward, dtype=torch.float)\n",
    "            next_state = torch.tensor(\n",
    "                transition_model.label.index((next_state_x, next_state_y)), dtype=torch.long\n",
    "                )\n",
    "            state_action = torch.tensor((state_x, state_y, action), dtype=torch.float)  # (x, y, a)\n",
    "\n",
    "            # Train transition model\n",
    "            optimizer_T.zero_grad()\n",
    "            pred_next_states = transition_model(state_action)\n",
    "            loss_T = nn.functional.cross_entropy(pred_next_states, next_state)\n",
    "            loss_T.backward()\n",
    "            optimizer_T.step()\n",
    "\n",
    "            # Train reward model\n",
    "            optimizer_R.zero_grad()\n",
    "            pred_rewards = reward_model(state_action)\n",
    "            loss_R = loss_fn(pred_rewards, reward)\n",
    "            loss_R.backward()\n",
    "            optimizer_R.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}: Loss_T = {loss_T.item()}, Loss_R = {loss_R.item()}\")\n",
    "\n",
    "    print(\"Model training completed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy(Q, state, action_size, epsilon=0.1):\n",
    "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.choice(action_size)  # Explore\n",
    "        return np.argmax(Q[state][i] for i in range(action_size))  # Exploit\n",
    "\n",
    "def q_learning(state, action, reward, next_state, Q, alpha, gamma, action_size):\n",
    "    \"\"\"Q-learning update rule.\"\"\"\n",
    "    best_next_action = np.argmax(Q[next_state[0]][next_state[1]][i] for i in range(action_size))\n",
    "    Q[state[0]][state[1]][action] += alpha * (reward + gamma * Q[next_state[0]][next_state[1]][best_next_action] - Q[state[0]][state[1]][action])\n",
    "\n",
    "def update_model(state, action, reward, next_state, transition_model, reward_model):\n",
    "    train_model_from_data([(state[0], state[1], action, reward, next_state[0], next_state[1])], transition_model, reward_model, epochs=2)\n",
    "\n",
    "def predict_model(state, action, transition_model, reward_model):\n",
    "    with torch.no_grad():\n",
    "        x = torch.tensor((state[0], state[1], action), dtype=torch.float)\n",
    "        transition_model.eval()\n",
    "        reward_model.eval()\n",
    "        new_state = transition_model(x)\n",
    "        new_state = transition_model.label[torch.argmax((new_state))]\n",
    "        reward = reward_model(x)\n",
    "\n",
    "    return reward, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dyna_q(env, recorded_trajectories, alpha=0.1, gamma=0.95, epsilon=0.1, planning_steps=10, episodes=500):\n",
    "    \"\"\"\n",
    "    Implements the Dyna-Q reinforcement learning algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "        env: The environment (assumed to follow OpenAI Gym-like API).\n",
    "        alpha: Learning rate.\n",
    "        gamma: Discount factor.\n",
    "        epsilon: Exploration probability.\n",
    "        planning_steps: Number of simulated updates per real step.\n",
    "        episodes: Number of episodes to train.\n",
    "\n",
    "    Returns:\n",
    "        Q-table (state-action values) and optimal policy.\n",
    "    \"\"\"\n",
    "    action_size = env2.action_space.n\n",
    "    # Initialize Q-table\n",
    "    Q = np.zeros((env2.height, env2.width, 2), dtype=('float64')  )\n",
    "    observed_state = []\n",
    "    for x, y, _, _, a, b   in recorded_trajectories:\n",
    "        if ([x, y] not in observed_state): observed_state.append([x,y])\n",
    "        if ([a, b] not in observed_state): observed_state.append([a,b])\n",
    "    recorded_trajectories = recorded_trajectories\n",
    "    # Model: Dictionary storing transitions {(state, action): (reward, next_state)}\n",
    "    transition_model = TransitionModel()\n",
    "    reward_model = RewardModel()\n",
    "    train_model_from_data(recorded_trajectories=recorded_trajectories, transition_model=transition_model, reward_model=reward_model)\n",
    "    done = False\n",
    "    # Training loop\n",
    "    counter = 0\n",
    "    while not done:\n",
    "        counter+=1\n",
    "        observation, _  = env.reset()\n",
    "        state = tuple(observation.get(\"agent\").get(\"pos\"))\n",
    "        if(state not in observed_state): observed_state.append(state)\n",
    "        action = eps_greedy(Q, state, action_size)\n",
    "        new_observation, reward, _, _, _ = env.step(action)\n",
    "        new_state = tuple(new_observation[\"agent\"][\"pos\"])\n",
    "        new_state = (int(new_state[0]), int(new_state[1]))\n",
    "        q_learning(state, action, reward, new_state, Q, alpha, gamma, action_size) \n",
    "        update_model(state, action, reward, new_state, transition_model, reward_model)\n",
    "        if(new_state not in observed_state): observed_state.append(new_state)\n",
    "        state=new_state\n",
    "        if(state == (14, 54)):\n",
    "            print(\"done\")\n",
    "            done = True\n",
    "\n",
    "        for imaginary_step in range(100):\n",
    "            imag_state = observed_state[np.random.choice(len(observed_state))]\n",
    "            imag_action = np.random.choice(action_size)\n",
    "            imag_reward, imag_new_state = predict_model(imag_state, imag_action, transition_model, reward_model)\n",
    "            q_learning(imag_state, imag_action, imag_reward, imag_new_state, Q=Q, alpha=alpha, action_size=action_size, gamma=gamma)\n",
    "        if(counter%100): print(counter)\n",
    "    # Derive optimal policy from Q-table\n",
    "    optimal_policy = np.argmax(Q, axis=1)\n",
    "    return Q, optimal_policy\n",
    "\n",
    "Q, optimal_policy = dyna_q(env=env, recorded_trajectories=trajectories)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inf266",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
